{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- STEP 1: LOAD AND CLEAN TRAINING DATA ---\n",
    "df = pd.read_csv('dat_train1.csv')\n",
    "df['event_timestamp'] = pd.to_datetime(df['event_timestamp'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates to ensure clean feature counts\n",
    "duplicate_mask = df.duplicated(subset=['customer_id', 'event_name', 'event_timestamp'], keep='first')\n",
    "df_clean = df[~duplicate_mask].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: CREATE LABELS (SUCCESS VS. LAPSE) ---\n",
    "# Identify Successful customers\n",
    "success_ids = set(df_clean[df_clean['event_name'] == 'order_shipped']['customer_id'])\n",
    "\n",
    "# Identify Lapsed customers (No order + inactive for 60 days)\n",
    "max_date = df_clean['event_timestamp'].max()\n",
    "last_event = df_clean.sort_values('event_timestamp').groupby('customer_id').tail(1)\n",
    "\n",
    "def get_label(row):\n",
    "    if row['customer_id'] in success_ids:\n",
    "        return 1 # Success\n",
    "    if (max_date - row['event_timestamp']) >= pd.Timedelta(days=60):\n",
    "        return 0 # Lapse\n",
    "    return -1 # Active/Exclude\n",
    "\n",
    "last_event['label'] = last_event.apply(get_label, axis=1)\n",
    "labels = last_event[last_event['label'] != -1][['customer_id', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: CREATE LABELS (SUCCESS VS. LAPSE) ---\n",
    "# Identify Successful customers\n",
    "success_ids = set(df_clean[df_clean['event_name'] == 'order_shipped']['customer_id'])\n",
    "\n",
    "# Identify Lapsed customers (No order + inactive for 60 days)\n",
    "max_date = df_clean['event_timestamp'].max()\n",
    "last_event = df_clean.sort_values('event_timestamp').groupby('customer_id').tail(1)\n",
    "\n",
    "def get_label(row):\n",
    "    if row['customer_id'] in success_ids:\n",
    "        return 1 # Success\n",
    "    if (max_date - row['event_timestamp']) >= pd.Timedelta(days=60):\n",
    "        return 0 # Lapse\n",
    "    return -1 # Active/Exclude\n",
    "\n",
    "last_event['label'] = last_event.apply(get_label, axis=1)\n",
    "labels = last_event[last_event['label'] != -1][['customer_id', 'label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: FLATTEN FEATURES FOR TRAINING ---\n",
    "df_clean['is_appl_submit'] = (df_clean['event_name'] == 'application_web_submit').astype(int)\n",
    "df_clean['is_add_to_cart'] = (df_clean['event_name'] == 'add_to_cart').astype(int)\n",
    "df_clean['is_browse'] = (df_clean['event_name'] == 'browse_products').astype(int)\n",
    "\n",
    "df_train_raw = df_clean.groupby('customer_id').agg(\n",
    "    total_actions=('event_name', 'count'),\n",
    "    has_applied=('is_appl_submit', 'max'),\n",
    "    max_items_in_cart=('is_add_to_cart', 'sum'),\n",
    "    num_unique_products=('is_browse', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Merge features with labels to create the final df_train\n",
    "df_train = df_train_raw.merge(labels, on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enhanced_features(df_input):\n",
    "    # Pre-calculate flags\n",
    "    df_input['is_appl_submit'] = (df_input['event_name'] == 'application_web_submit').astype(int)\n",
    "    df_input['is_add_to_cart'] = (df_input['event_name'] == 'add_to_cart').astype(int)\n",
    "    df_input['is_browse'] = (df_input['event_name'] == 'browse_products').astype(int)\n",
    "    \n",
    "    features = df_input.groupby('customer_id').agg(\n",
    "        total_actions=('event_name', 'count'),\n",
    "        has_applied=('is_appl_submit', 'max'),\n",
    "        total_carts=('is_add_to_cart', 'sum'),\n",
    "        total_browses=('is_browse', 'sum'),\n",
    "        first_action=('event_timestamp', 'min'),\n",
    "        last_action=('event_timestamp', 'max')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate Duration in minutes (add 0.1 to avoid div by zero)\n",
    "    features['duration_min'] = (features['last_action'] - features['first_action']).dt.total_seconds() / 60.0 + 0.1\n",
    "    \n",
    "    # --- THE KEY RATIOS ---\n",
    "    # Velocity: How fast are they clicking? (High velocity = High intent)\n",
    "    features['action_velocity'] = features['total_actions'] / features['duration_min']\n",
    "    \n",
    "    # Cart Consistency: Are they browsing just to browse, or are they adding?\n",
    "    features['cart_per_browse'] = features['total_carts'] / (features['total_browses'] + 1)\n",
    "    \n",
    "    # Application Intent: If they applied, how many actions did it take to get there?\n",
    "    features['actions_to_app'] = features['total_actions'] * features['has_applied']\n",
    "\n",
    "    return features.drop(columns=['first_action', 'last_action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PRE-STEP: SETUP FLAGS & IDENTIFY SUCCESS ---\n",
    "# Ensure flags exist before we cut/copy\n",
    "df_clean['is_appl_submit'] = (df_clean['event_name'] == 'application_web_submit').astype(int)\n",
    "df_clean['is_add_to_cart'] = (df_clean['event_name'] == 'add_to_cart').astype(int)\n",
    "df_clean['is_browse'] = (df_clean['event_name'] == 'browse_products').astype(int)\n",
    "\n",
    "# Identify Success IDs (Global Truth)\n",
    "success_ids = df_clean[df_clean['event_name'] == 'order_shipped']['customer_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: DEFINE THE \"END BOUNDARY\" FOR EVERYONE ---\n",
    "# For Success: The journey effectively ends at 'order_shipped'\n",
    "# For Failure: The journey effectively ends at their last observed event\n",
    "journey_bounds = df_clean.groupby('customer_id').agg(\n",
    "    start_time=('event_timestamp', 'min'),\n",
    "    last_seen=('event_timestamp', 'max')\n",
    ")\n",
    "\n",
    "# Isolate the specific timestamp of success (order_shipped)\n",
    "success_events = df_clean[df_clean['event_name'] == 'order_shipped'][['customer_id', 'event_timestamp']]\n",
    "success_events = success_events.rename(columns={'event_timestamp': 'success_time'})\n",
    "\n",
    "# Merge success times into bounds\n",
    "journey_bounds = journey_bounds.merge(success_events, on='customer_id', how='left')\n",
    "\n",
    "# Define 'effective_end': Use success_time if available, else use last_seen\n",
    "journey_bounds['effective_end'] = journey_bounds['success_time'].fillna(journey_bounds['last_seen'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: GENERATE RANDOM CUT TIMES ---\n",
    "np.random.seed(42)\n",
    "\n",
    "# Vectorized Random Generation (Much faster than .apply)\n",
    "# We convert to int64 (nanoseconds) to generate random integers\n",
    "journey_bounds['start_ns'] = journey_bounds['start_time'].astype('int64')\n",
    "journey_bounds['end_ns'] = journey_bounds['effective_end'].astype('int64')\n",
    "\n",
    "# Generate a random point between Start and Effective End\n",
    "# (If start == end, it just picks that time)\n",
    "journey_bounds['cut_ns'] = journey_bounds.apply(\n",
    "    lambda row: np.random.randint(row['start_ns'], row['end_ns']) \n",
    "    if row['start_ns'] < row['end_ns'] else row['start_ns'], axis=1\n",
    ")\n",
    "\n",
    "journey_bounds['cut_time'] = pd.to_datetime(journey_bounds['cut_ns'], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: APPLY THE CUT (FILTERING) ---\n",
    "# Merge the specific cut time back to the main event log\n",
    "df_aug = df_clean.merge(journey_bounds[['customer_id', 'cut_time']], on='customer_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enhanced_features(df_input):\n",
    "    # Pre-calculate flags\n",
    "    df_input['is_appl_submit'] = (df_input['event_name'] == 'application_web_submit').astype(int)\n",
    "    df_input['is_add_to_cart'] = (df_input['event_name'] == 'add_to_cart').astype(int)\n",
    "    df_input['is_browse'] = (df_input['event_name'] == 'browse_products').astype(int)\n",
    "    \n",
    "    features = df_input.groupby('customer_id').agg(\n",
    "        total_actions=('event_name', 'count'),\n",
    "        has_applied=('is_appl_submit', 'max'),\n",
    "        total_carts=('is_add_to_cart', 'sum'),\n",
    "        total_browses=('is_browse', 'sum'),\n",
    "        first_action=('event_timestamp', 'min'),\n",
    "        last_action=('event_timestamp', 'max')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate Duration in minutes (add 0.1 to avoid div by zero)\n",
    "    features['duration_min'] = (features['last_action'] - features['first_action']).dt.total_seconds() / 60.0 + 0.1\n",
    "    \n",
    "    # --- THE KEY RATIOS ---\n",
    "    # Velocity: How fast are they clicking? (High velocity = High intent)\n",
    "    features['action_velocity'] = features['total_actions'] / features['duration_min']\n",
    "    \n",
    "    # Cart Consistency: Are they browsing just to browse, or are they adding?\n",
    "    features['cart_per_browse'] = features['total_carts'] / (features['total_browses'] + 1)\n",
    "    \n",
    "    # Application Intent: If they applied, how many actions did it take to get there?\n",
    "    features['actions_to_app'] = features['total_actions'] * features['has_applied']\n",
    "\n",
    "    return features.drop(columns=['first_action', 'last_action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: GENERATE BLINDED TRAINING DATA ---\n",
    "# Using the cutoff logic to simulate \"live\" journeys for successful customers\n",
    "df_train_truncated = df_aug[df_aug['event_timestamp'] <= df_aug['cut_time']].copy()\n",
    "\n",
    "# --- STEP 2: AGGREGATE USING ENHANCED FEATURES ---\n",
    "# We use the function to create 'velocity' and 'ratios'\n",
    "df_train_enhanced = extract_enhanced_features(df_train_truncated)\n",
    "\n",
    "# --- STEP 3: FILTER OUT \"IN-BETWEEN\" USERS ---\n",
    "# Only train on confirmed Successes or confirmed 60-day Lapsers\n",
    "final_lapsed_ids = labels[labels['label'] == 0]['customer_id']\n",
    "final_success_ids = labels[labels['label'] == 1]['customer_id']\n",
    "valid_ids = pd.concat([final_lapsed_ids, final_success_ids])\n",
    "\n",
    "df_train_final = df_train_enhanced[df_train_enhanced['customer_id'].isin(valid_ids)].copy()\n",
    "\n",
    "# --- STEP 4: ATTACH LABELS ---\n",
    "df_train_final['label'] = df_train_final['customer_id'].isin(success_ids).astype(int)\n",
    "\n",
    "# --- STEP 5: PREPARE THE TEST SET ---\n",
    "# Apply the EXACT same enhanced features to the open journeys\n",
    "X_test_enhanced = extract_enhanced_features(df_open_raw)\n",
    "\n",
    "# Ensure X_test has the same columns as our training features (excluding label/id)\n",
    "features_list = [col for col in df_train_final.columns if col not in ['customer_id', 'label']]\n",
    "X_test = X_test_enhanced[features_list].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Samples: 1235180\n",
      "Target Distribution:\n",
      "label\n",
      "0    0.774735\n",
      "1    0.225265\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 5: FILTER AND LABEL ---\n",
    "\n",
    "# 1. Identify valid IDs (Successes + 60-day Lapsers only)\n",
    "final_lapsed_ids = labels[labels['label'] == 0]['customer_id']\n",
    "final_success_ids = labels[labels['label'] == 1]['customer_id']\n",
    "valid_ids = pd.concat([final_lapsed_ids, final_success_ids])\n",
    "\n",
    "# 2. Filter the aggregated features to only include these valid customers\n",
    "df_train_final = df_train_final[df_train_final['customer_id'].isin(valid_ids)].copy()\n",
    "\n",
    "# 3. Attach the final label (1 for Success, 0 for Lapse)\n",
    "df_train_final['label'] = df_train_final['customer_id'].isin(success_ids).astype(int)\n",
    "\n",
    "# --- STEP 6: FINAL CHECK ---\n",
    "print(f\"Final Training Samples: {len(df_train_final)}\")\n",
    "print(f\"Target Distribution:\\n{df_train_final['label'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training features are everything except 'customer_id' and 'label'\n",
    "features_list = [col for col in df_train_final.columns if col not in ['customer_id', 'label']]\n",
    "\n",
    "# Create the final X_test using the same columns\n",
    "X_test = X_test_safe[features_list].copy()\n",
    "\n",
    "# Ensure we handle any NaNs in the test set (just in case)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X and y\n",
    "X = df_train_final[features_list]\n",
    "y = df_train_final['label']\n",
    "\n",
    "# Split to validate before submitting\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 1. The Base Model: Keeping it shallow for stability\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=3,           # Shallow depth is better for LogLoss here\n",
    "    learning_rate=0.05,    # Slow learning avoids overshooting\n",
    "    n_estimators=300,\n",
    "    objective='binary:logistic',\n",
    "    random_state=42,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8\n",
    ")\n",
    "\n",
    "# 2. The Calibration: The secret to the 0.042 range\n",
    "# This adjusts the probabilities to be more \"honest\"\n",
    "calibrated_xgb = CalibratedClassifierCV(xgb_model, method='isotonic', cv=5)\n",
    "calibrated_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict Probabilities\n",
    "# [:, 1] gets the probability of \"Success\"\n",
    "test_probs = calibrated_xgb.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file ready!\n"
     ]
    }
   ],
   "source": [
    "# Load the submission template to get the exact ID order\n",
    "sub_template = pd.read_csv('open_journeys1_flattened_all0.csv')\n",
    "\n",
    "# Create a dictionary of our predictions for easy mapping\n",
    "pred_dict = dict(zip(X_test_safe['customer_id'], test_probs))\n",
    "\n",
    "# Map our predictions back to the template's 'id' column\n",
    "# This ensures the order and format are 100% correct for the leaderboard\n",
    "sub_template['order_shipped'] = sub_template['id'].map(pred_dict)\n",
    "\n",
    "# Important: If any IDs didn't match, fill them with the global mean \n",
    "# (though your logic should cover everyone)\n",
    "sub_template['order_shipped'] = sub_template['order_shipped'].fillna(y.mean())\n",
    "\n",
    "sub_template.to_csv('submission_v1_calibrated.csv', index=False)\n",
    "\n",
    "print(\"Submission file ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
