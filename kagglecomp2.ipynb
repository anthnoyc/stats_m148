{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# 1. DEFINE A SCHEMA\n",
    "# Loading 'event_name' as a category immediately saves ~80% RAM\n",
    "# We skip loading 'customer_id' if it's already in the file, or use only what's needed\n",
    "dtypes = {\n",
    "    'id': 'str',\n",
    "    'event_name': 'category'\n",
    "}\n",
    "\n",
    "# 2. LOAD INDIVIDUALLY AND CLEAN\n",
    "def load_and_clean(file_path):\n",
    "    # Load with dtypes to save memory instantly\n",
    "    df = pd.read_csv(file_path, dtype=dtypes)\n",
    "    \n",
    "    # Process datetime immediately to free up string memory\n",
    "    df['event_timestamp'] = pd.to_datetime(df['event_timestamp'], utc=True, errors='coerce')\n",
    "    \n",
    "    # Process IDs without creating intermediate lists\n",
    "    # .str.extract is often more memory-efficient than .split for large data\n",
    "    df['customer_id'] = df['id'].str.extract(r'^([^\\s]+)', expand=False)\n",
    "    \n",
    "    gc.collect() # Clean artifacts from extraction\n",
    "    return df\n",
    "\n",
    "# Process one at a time\n",
    "df_train2 = load_and_clean('dat_train2.csv')\n",
    "gc.collect()\n",
    "\n",
    "df_test2 = load_and_clean('open_journeys2.csv')\n",
    "gc.collect()\n",
    "\n",
    "# 3. IDENTIFY SUCCESS (REMAINING LOGIC)\n",
    "success_ids = df_train2.loc[df_train2['event_name'] == 'order_shipped', 'id'].unique()\n",
    "success_set = set(success_ids) # O(1) lookup speed\n",
    "\n",
    "print(f\"Setup Complete. Training rows: {len(df_train2)}\")\n",
    "gc.collect() # Clear temporary memory artifacts\n",
    "\n",
    "# --- STEP 2: VECTORIZED LABELING (FIXED FOR MEMORY) ---\n",
    "\n",
    "# 1. Get the last event timestamp ONLY for unique IDs\n",
    "# Better than transform('max') here because it creates a smaller object\n",
    "journey_end_times = df_train2.groupby('id')['event_timestamp'].max()\n",
    "\n",
    "# 2. Identify the 'Present Day'\n",
    "max_date = df_train2['event_timestamp'].max()\n",
    "\n",
    "# 3. Create a dedicated Labels DataFrame (Much smaller than the full event log)\n",
    "train_labels = pd.DataFrame({'id': df_train2['id'].unique()})\n",
    "\n",
    "# 4. Map the end times and calculate days since last event for everyone\n",
    "train_labels['last_event'] = train_labels['id'].map(journey_end_times)\n",
    "train_labels['days_since_last'] = (max_date - train_labels['last_event'])\n",
    "\n",
    "# 5. Vectorized Logical Conditions\n",
    "is_success = train_labels['id'].isin(success_set)\n",
    "is_lapsed = train_labels['days_since_last'] >= pd.Timedelta(days=60)\n",
    "\n",
    "# 6. Apply Labels (1 = Success, 0 = Lapsed, -1 = Still Active)\n",
    "# np.select is much faster than apply() for large columns\n",
    "train_labels['label'] = np.select(\n",
    "    [is_success, is_lapsed], \n",
    "    [1, 0], \n",
    "    default=-1\n",
    ")\n",
    "\n",
    "# 7. Final Training Filter\n",
    "# Keep only confirmed wins and losses; discard journeys still in progress\n",
    "train_labels = train_labels[train_labels['label'] != -1][['id', 'label']].copy()\n",
    "\n",
    "print(f\"Labeling complete. Found {len(train_labels)} valid journeys for training.\")\n",
    "print(f\"Success Rate: {train_labels['label'].mean():.2%}\")\n",
    "\n",
    "# Memory cleanup of temporary objects\n",
    "del journey_end_times\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncation complete. Reduced rows from 59M to 39244358\n"
     ]
    }
   ],
   "source": [
    "# random cutoff logic\n",
    "\n",
    "# 1. IDENTIFY JOURNEY BOUNDARIES\n",
    "# We need to know when each journey started and when it \"effectively\" ended\n",
    "journey_bounds = df_train2.groupby('id')['event_timestamp'].agg(['min', 'max']).reset_index()\n",
    "journey_bounds.columns = ['id', 'start_time', 'end_time']\n",
    "\n",
    "# 2. GENERATE RANDOM CUTOFFS\n",
    "# We want a random time between the first event and the last event\n",
    "np.random.seed(42)\n",
    "\n",
    "# Convert to nanoseconds for easy integer-based random sampling\n",
    "start_ns = journey_bounds['start_time'].astype('int64')\n",
    "end_ns = journey_bounds['end_time'].astype('int64')\n",
    "\n",
    "# Generate the cut time\n",
    "journey_bounds['cut_ns'] = start_ns + (np.random.rand(len(journey_bounds)) * (end_ns - start_ns)).astype('int64')\n",
    "journey_bounds['cut_time'] = pd.to_datetime(journey_bounds['cut_ns'], utc=True)\n",
    "\n",
    "# 3. APPLY THE CUTOFF TO THE DATA\n",
    "# Merge the cut_time back to the main dataframe\n",
    "df_train2 = df_train2.merge(journey_bounds[['id', 'cut_time']], on='id', how='left')\n",
    "\n",
    "# Keep only events that happened BEFORE the random cut-off\n",
    "df_train_truncated = df_train2[df_train2['event_timestamp'] <= df_train2['cut_time']].copy()\n",
    "\n",
    "# 4. CLEAN UP\n",
    "# We no longer need the full 60M row dataframe or the bounds\n",
    "del df_train2, journey_bounds\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Truncation complete. Reduced rows from 59M to {len(df_train_truncated)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening Truncated Training Data...\n",
      "Flattening Test Data...\n",
      "Training Matrix Shape: (1450231, 9)\n",
      "Test Matrix Shape: (158325, 8)\n"
     ]
    }
   ],
   "source": [
    "def extract_elite_features_v4(df_input):\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # 1. Handle the \"Reference Time\" (The Cutoff)\n",
    "    # If cut_time isn't there, use the journey's own last event time\n",
    "    if 'cut_time' not in df.columns:\n",
    "        df['reference_time'] = df.groupby('id')['event_timestamp'].transform('max')\n",
    "    else:\n",
    "        df['reference_time'] = df['cut_time']\n",
    "    \n",
    "    # 2. Pre-calculate Intent Flags\n",
    "    df['is_cart'] = (df['event_name'] == 'add_to_cart').astype(int)\n",
    "    \n",
    "    # 3. Recency Decay (Lambda 0.04)\n",
    "    # Use reference_time instead of cut_time to avoid KeyErrors\n",
    "    df['hrs_ago'] = (df['reference_time'] - df['event_timestamp']).dt.total_seconds() / 3600.0\n",
    "    df['weight'] = np.exp(-0.04 * df['hrs_ago'])\n",
    "    df['w_cart'] = df['is_cart'] * df['weight']\n",
    "    \n",
    "    # 4. Aggregation\n",
    "    features = df.groupby('id').agg(\n",
    "        recency_score=('weight', 'sum'),\n",
    "        weighted_carts=('w_cart', 'sum'),\n",
    "        total_acts=('event_name', 'count'),\n",
    "        duration_hrs=('hrs_ago', 'max'),\n",
    "        event_variety=('event_name', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 5. Feature Engineering\n",
    "    features['velocity'] = features['total_acts'] / (features['duration_hrs'] + 0.1)\n",
    "    features['cart_density'] = features['weighted_carts'] / (features['total_acts'] + 1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# --- EXECUTION ---\n",
    "print(\"Flattening Truncated Training Data...\")\n",
    "X_train_full = extract_elite_features_v4(df_train_truncated)\n",
    "df_final_train = X_train_full.merge(train_labels, on='id', how='inner')\n",
    "\n",
    "print(\"Flattening Test Data...\")\n",
    "X_test_final = extract_elite_features_v4(df_test2)\n",
    "\n",
    "# Memory Cleanup\n",
    "del X_train_full\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Training Matrix Shape: {df_final_train.shape}\")\n",
    "print(f\"Test Matrix Shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost Model...\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# 1. Define Features (Excluding ID and Label)\n",
    "features_list = ['recency_score', 'weighted_carts', 'total_acts', \n",
    "                 'duration_hrs', 'event_variety', 'velocity', 'cart_density']\n",
    "\n",
    "X = df_final_train[features_list]\n",
    "y = df_final_train['label']\n",
    "\n",
    "# 2. Train Ultra-Conservative XGBoost\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,           # Shallow trees prevent overfitting\n",
    "    learning_rate=0.05,\n",
    "    gamma=10,              # High penalty for making complex splits\n",
    "    reg_lambda=50,         # Strong L2 regularization\n",
    "    min_child_weight=20,   # Requires more proof before creating a leaf\n",
    "    objective='binary:logistic',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost Model...\")\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Generate Raw Test Probabilities\n",
    "test_probs_raw = model.predict_proba(X_test_final[features_list])[:, 1]\n",
    "\n",
    "# 4. The 0.041 Probability Calibration\n",
    "# Squash the probabilities to handle the sparse test set\n",
    "test_probs_squashed = np.power(test_probs_raw, 1.5)\n",
    "\n",
    "# Shift the mean to a pessimistic target (e.g., 0.035)\n",
    "target_mean = 0.035\n",
    "final_probs = test_probs_squashed * (target_mean / test_probs_squashed.mean())\n",
    "\n",
    "# Final safety clip\n",
    "final_probs = np.clip(final_probs, 0.0001, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the official Kaggle template\n",
    "df_kaggle_template = pd.read_csv('open_journeys2_flattened_all0.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Mean: 0.0161\n",
      "Max Prob: 0.1500\n",
      "Min Prob: 0.0011\n",
      "\n",
      "Successfully saved to xgboost_comp2_v1.csv\n"
     ]
    }
   ],
   "source": [
    "# 2. Create a lookup Series from your predictions\n",
    "# We use the 'id' from X_test_final as the index for a quick lookup\n",
    "preds_series = pd.Series(final_probs, index=X_test_final['id'].astype(str))\n",
    "\n",
    "# 3. Map predictions to the template\n",
    "# We use .map() to match the 'id' column in the template to our predictions\n",
    "# .fillna(0.015) is a safety net for any IDs that had 0 events in the test log\n",
    "df_kaggle_template['order_shipped'] = df_kaggle_template['id'].map(preds_series).fillna(0.015)\n",
    "\n",
    "# 4. Final Verification\n",
    "print(f\"Submission Mean: {df_kaggle_template['order_shipped'].mean():.4f}\")\n",
    "print(f\"Max Prob: {df_kaggle_template['order_shipped'].max():.4f}\")\n",
    "print(f\"Min Prob: {df_kaggle_template['order_shipped'].min():.4f}\")\n",
    "\n",
    "# 5. Save the file\n",
    "submission_name = 'xgboost_comp2_v1.csv'\n",
    "df_kaggle_template[['id', 'order_shipped']].to_csv(submission_name, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved to {submission_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/calebtran/Desktop/STATS M148/kagglecomp2.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calebtran/Desktop/STATS%20M148/kagglecomp2.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calebtran/Desktop/STATS%20M148/kagglecomp2.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Get feature importance\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/calebtran/Desktop/STATS%20M148/kagglecomp2.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m importance \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfeature_importances_\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calebtran/Desktop/STATS%20M148/kagglecomp2.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m feat_imp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(importance, index\u001b[39m=\u001b[39mfeatures_list)\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/calebtran/Desktop/STATS%20M148/kagglecomp2.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Plot\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance\n",
    "importance = model.feature_importances_\n",
    "feat_imp = pd.Series(importance, index=features_list).sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "feat_imp.plot(kind='barh', color='teal')\n",
    "plt.title('Which Features are Driving the 0.0407 Score?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
